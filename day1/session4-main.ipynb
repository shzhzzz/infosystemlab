{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flowers Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__scikit-learn__ (`sklearn`) is a Python package which contains a large number of functions for doing machine learning, including classification.\n",
    "<br>\n",
    "It contains many subpackages. In the code below we import the necessary subpackages when they are needed.\n",
    "\n",
    "__scikit-learn__（`sklearn`）は、分類など機械学習を実行するための多数の関数を含むPythonパッケージです。\n",
    "<br>\n",
    "多くのサブパッケージが含まれています。以下のコードでは、必要なサブパッケージを適宜にインポートします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset / アヤメのデータセット\n",
    "\n",
    "### Observations and labels / 観測とラベル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the \"iris dataset\" with 150 records and five attributes: flower type, sepal length, sepal width, petal length, petal width.\n",
    "\n",
    "150のサンプルと5つの属性（花の種類、がくの長さ、がくの幅、花びらの長さ、花びらの幅）を持つ「アヤメのデータセット」を使用します。\n",
    "<br>\n",
    "<img src=\"./img/petal-sepal.jpg\" width=\"150\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = np.genfromtxt('iris_data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample is composed of:\n",
    "- A flower type 0 (iris setosa), 1 (iris versicolor) or 2 (iris virginica) ⇒ labels\n",
    "- The 4 sepal and petal measurements ⇒ observations\n",
    "\n",
    "各サンプルは次のもので構成されています。\n",
    "- 花の種類：0（ヒオウギアヤメ）、1（ブルーフラッグ）または 2（バージニカ） ⇒ ラベル\n",
    "- 花びらとがくの4つの測定データ ⇒ 観察"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the remainder, we will use the letters `X` for the observations and `y` for the labels.\n",
    "\n",
    "これ以降は、観測には文字`X`を使用し、ラベルには`y`を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[:,1:]\n",
    "y = iris[:,0]\n",
    "print(\"X has shape:\", X.shape)\n",
    "print(\"y has shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation / データの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting into training and testing set / トレーニングとテストセットに分割\n",
    "\n",
    "Most times a classifier is trained in order to correctly predict labels for _new future observations_.\n",
    "<br>\n",
    "Consequently, what really matters is the performance of the classifier when presented with new obervations.\n",
    "<br>\n",
    "A practical way to measure future performance is to split the available examples in two sets:\n",
    "- A set that is used for training ⇒ __training set__\n",
    "- A set that is used for testing the performance ⇒ __testing set__\n",
    "\n",
    "\n",
    "ほとんどの場合、分類器をトレーニングさせるり用は_新しい将来の観測_のラベルを正しく予測するためです。\n",
    "<br>\n",
    "そのため、本当に重要なのは、新しい観測を提示したときの分類器のパフォーマンスです。\n",
    "<br>\n",
    "将来のパフォーマンスを測定する実際的な方法は、使用可能な例を2つのセットに分割することです。\n",
    "- トレーニングに使用されるセット ⇒ __トレーニングセット__\n",
    "- パフォーマンスのテストに使用されるセット ⇒ __テストセット__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples in the testing set are playing the role of the \"never seen before\" observations.\n",
    "<br>\n",
    "Computing the accuracy on the testing set gives us an idea of how well the trained classifier __generalizes__, i.e. how well it classifies observations that are not part of the training set.\n",
    "\n",
    "テストセットは、「これまでに見たことのない」観測の役割を果たしています。\n",
    "<br>\n",
    "テストセットの精度を計算すると、トレーニングされた分類器がどの程度__一般化__するか（つまり、トレーニングセットに入ってない観測をどの程度適切に分類でいるか）がわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us divide the dataset into a ____ and a __testing__ set.\n",
    "<br>\n",
    "The samples will be _randomly_ associated to the training or the testing set.\n",
    "\n",
    "まず、データセットを__トレーニングセット__と__テストセット__に分割しましょう。\n",
    "<br>\n",
    "サンプルは、トレーニングまたはテストセットに_ランダムに_割り当てられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_▶ Using sklearn to split data / sklearnを使用したデータの分割_\n",
    "\n",
    "We use the function `train_test_split` from `sklearn.model_selection` to split the data into training and testing sets.\n",
    "\n",
    "`sklearn.model_selection`の`train_test_split`関数を使用して、データをトレーニングセットとテストセットに分割できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# test_size=0.3 ⇒ 30％ of data in test set, the rest (70%) in training set\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Testing set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many flowers of each species are in the training set and the testing set.\n",
    "\n",
    "トレーニングセットとテストセットに含まれる各種の花の数を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training set:\")\n",
    "print(\"Iris setosa:\", np.sum(y_train == 0))\n",
    "print(\"Iris versicolor:\", np.sum(y_train == 1))\n",
    "print(\"Iris virginica:\", np.sum(y_train == 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"testing set:\")\n",
    "print(\"Iris setosa:\", np.sum(y_test == 0))\n",
    "print(\"Iris versicolor:\", np.sum(y_test == 1))\n",
    "print(\"Iris virginica:\", np.sum(y_test == 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_▶ Manually splitting data / 手動のデータ分割 (optional)_\n",
    "\n",
    "For reference, here is the code for splitting the data manually. It does the same thing as `train_test_split`.\n",
    "<br>\n",
    "(If you want to run the code, first transform the cell into a code cell.)\n",
    "\n",
    "参考までに、データを手動で分割するコードを次に示します。`train_test_split`と同じことを行います。\n",
    "<br>\n",
    "（コードを実行したい場合は、まずセルをコードセルに変換してください。）"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Create a permutation of the indicies\n",
    "number_samples = X.shape[0]\n",
    "shuffle_index = np.random.permutation(number_samples)\n",
    "\n",
    "# use the permuted list as indices\n",
    "X = X[shuffle_index]\n",
    "y = y[shuffle_index]\n",
    "\n",
    "# Split the data in training and testing\n",
    "testing_ratio = 0.3 # 30%\n",
    "test_samples = int(testing_ratio * number_samples)\n",
    "\n",
    "# from 0 to test_samples-1\n",
    "X_test = X[:test_samples]\n",
    "y_test = y[:test_samples]\n",
    "\n",
    "# From test_samples to end\n",
    "X_train = X[test_samples:]\n",
    "y_train = y[test_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data standardization / データの標準化\n",
    "\n",
    "The goal of standardization is to transform the features so that they have zero mean and unit variance. This is important to do to prevent that some features have larger influence on the result than others. \n",
    "\n",
    "標準化の目的は、平均と単位分散がゼロになるように特徴を変換することです。これは、一部の機能が他の機能よりも結果に大きな影響を与えることを防ぐために行うことが重要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package `sklearn.preprocessing` provides a `StandardScaler` that computes the scaling parameters from a training set and makes it possible to easily apply these parameters to new data. This is the function we will usually use for scaling.\n",
    "\n",
    "パッケージ`sklearn.preprocessing`は、トレーニングセットからスケーリングパラメーターを計算し、これらのパラメーターを新しいデータに適用できるようにする`StandardScaler`を提供します。標準化を行うために使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the parameters of the `StandardScaler` have to be computed from the training set. This is done with the method `fit`.\n",
    "\n",
    "まず、`StandardScaler`のパラメーターをトレーニングセットから計算する必要があります。これはメソッド`fit`で行われます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can create a scaled training set using the method `transform`.\n",
    "\n",
    "次に、`transform`メソッドを使用して、スケーリングされたトレーニングセットを作成できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = X_train # We keep a copy of the original training set for plotting\n",
    "X_train = scaler.transform(X_train_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the scaled data looks like.\n",
    "\n",
    "スケーリングされたデータがどのように見えるか見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train_raw[:,0], 'b-', label=\"Sepal length\")\n",
    "plt.plot(X_train[:,0], 'b--', label=\"Sepal length scaled\")\n",
    "plt.plot(X_train_raw[:,1], 'r-', label=\"Sepal width\")\n",
    "plt.plot(X_train[:,1], 'r--', label=\"Sepal width scaled\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to scale also the observations in the test set.\n",
    "\n",
    "テストセットの観測値もスケーリングする必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_raw = X_test\n",
    "X_test = scaler.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> IMPORTANT: Fit the scaler to the _training data only_, not to the full dataset (including the test set). Only then use it to transform both the training set and the test set.\n",
    "\n",
    ">重要：fit関数は全部のデータセットではなく、_トレーニングデータのみ_に適合させます。それを使用してトレーニングセットとテストセットの両方を変換します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try  it yourself ! / 自分で試そう！\n",
    "\n",
    "[Click here](session4-playground1.ipynb) to open a sample notebook and try doing data processing\n",
    "\n",
    "[ここをクリックして](session4-playground1.ipynb)、サンプルのノートブックを開き、データ処理を行ってみてください\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using SVMs / SVMを使った分類\n",
    "\n",
    "**Support Vector Machines (SVMs)** are a set of often used supervised learning algorithms. <br>\n",
    "(In this practical course we do not go into the much theoretical details about the used methods. For a quick overview of SVMs see the [wikipedia page](https://en.wikipedia.org/wiki/Support-vector_machine).)\n",
    "\n",
    "サポートベクターマシン（SVM）は、よく使用される教師あり学習アルゴリズムです。\n",
    "<br>\n",
    "（この実習では、使用される方法に関する理論的な詳細については学びません。SVMの概要については、[wikipediaページ](https://en.wikipedia.org/wiki/Support-vector_machine)などを参照してください。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the `LinearSVC` classifer from the package `sklearn.svm`.\n",
    "<br>\n",
    "__SVC__ in the classifier name stands for Support Vector Classification; `LinearSVC` is an SVM based classifier using a linear model.\n",
    "\n",
    "パッケージ`sklearn.svm`から`LinearSVC`分類器をインポートしましょう。\n",
    "<br>\n",
    "分類器名の__SVC__は、Support Vector Classification（サポートベクター分類）を表します。`LinearSVC`は、線形モデルを使用したSVMの分類器です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LinearSVC` is based on the following linear model:\n",
    "$y_\\text{prediction} = \\text{f}(\\mathbf{W} \\mathbf{X} + \\mathbf{b})$\n",
    "<br>\n",
    "Namely, the prediction is a weighted sum of the observations plus a bias ($\\mathbf{W} \\mathbf{X} + \\mathbf{b}$), that goes through a decision function $\\text{f}(\\cdot)$.\n",
    "\n",
    "`LinearSVC`は次の線形モデルに基づいています：\n",
    "$y_\\text{prediction} = \\text{f}(\\mathbf{W} \\mathbf{X} + \\mathbf{b})$\n",
    "<br>\n",
    "つまり、予測は観測値とバイアス（$\\mathbf{W} \\mathbf{X} + \\mathbf{b}$）の加重和であり、和の結果は決定関数$\\text{f}(\\cdot)$を通ります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the classifier do the following.\n",
    "\n",
    "分類器を作成するには、次を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_SVC = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the training is to find a set of parameters such that the classifier is able to predict the label associated to the feature for the training set.\n",
    "<br>\n",
    "The `LinearSVC` provides the method `fit` that does it for us:\n",
    "\n",
    "トレーニングの目的は、分類器がトレーニングセットの特徴に関連付けられたラベルを予測できるように、適宜なパラメータを見つけることです。\n",
    "<br>\n",
    "`LinearSVC`はそれを行うためのメソッド` fit`を提供しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_SVC.fit(X_train, y_train); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` method tunes the parameter of the classifier `(w,b)`.\n",
    "<br>\n",
    "It goes through the training set and updates the weights $\\mathbf{W}$ and bias $\\mathbf{b}$  of the classifier in order to classify the samples.\n",
    "\n",
    "`fit`メソッドは分類器のパラメーター`(w,b)`を調整します。\n",
    "<br>\n",
    "サンプルを分類するために、トレーニングセットを使用し、分類器の重み$\\mathbf{W}$とバイアス$\\mathbf{b}$の値を更新します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the classifier has been trained on the training set, let us use the testing set to check the performance. First we will try it on a single sample.\n",
    "<br>\n",
    "The `predict` function gives the predicted value.\n",
    "\n",
    "\n",
    "分類器がトレーニングセットでトレーニングされたあと、テストセットを使用してパフォーマンスを確認します。まず、1つだけのサンプルで試してみましょう。\n",
    "<br>\n",
    "予測は`predict`関数を使用して行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "# The ith test sample is composed of\n",
    "# the label\n",
    "y_i = y_test[i]\n",
    "# the features (reshape is used because predict needs a 2D array)\n",
    "X_i = X_test[i].reshape(1, -1)\n",
    "\n",
    "print(y_i, X_i)\n",
    "\n",
    "# Show the truth\n",
    "print(\"True Flower type:\", y_i)\n",
    "\n",
    "# Using predict method\n",
    "# the input X_i is an array with a single feature\n",
    "# the result is an array with a single element\n",
    "y_pred = linear_SVC.predict(X_i)\n",
    "y_pred = y_pred[0]\n",
    "\n",
    "print(\"Prediction:\", y_pred)\n",
    "\n",
    "# Check the prediction\n",
    "if y_pred == y_i:\n",
    "    print(\"This is a correct prediction\")\n",
    "else:\n",
    "    print(\"This is not a correct prediction\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` function can be applied to the whole testing set in one call.\n",
    "\n",
    "`predict`関数は、一括にテストセット全体に適用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linear_SVC.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can compare the prediction to the labels of the test set.\n",
    "\n",
    "次に、予測結果をテストセットのラベルと比較できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the prediction to the truth\n",
    "res = (y_pred == y_test)\n",
    "\n",
    "print(y_pred.shape, y_test.shape)\n",
    "\n",
    "# Show a few examples\n",
    "for i in range(10):\n",
    "    print(\"True labels:\", y_test[i],  \"\\t Predicted labels:\", y_pred[i], \"\\tCorrect prediction:\", res[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification accuracy / 分類精度\n",
    "\n",
    "We would like to know if the classifier performs \"well enough\".\n",
    "<br>\n",
    "Thus, we need a way to measure the __performance__ of the classifier.\n",
    "\n",
    "One simple measure is to determine the proportion of the predictions made by the classifier that are correct. This is called the __accuracy__ of the classifier.\n",
    "\n",
    "分類器が「十分に機能する」かどうかを確認したいです。\n",
    "<br>\n",
    "したがって、分類子の__パフォーマンス__を測定する方法が必要です。\n",
    "\n",
    "簡単な方法の1つは、分類器によって行われた予測のうち、正しいものの割合を決定することです。これは、分類器の__精度__と呼ばれます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` provides a function to compute the accuracy.\n",
    "\n",
    "`sklearn`は精度を計算する関数を提供します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "A = accuracy_score(y_test,  y_pred)\n",
    "print(\"Accuracy: {:.02f}\".format(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Confusion matrix / 混同行列\n",
    "\n",
    "A convenient representation of the classification result is the __confusion matrix__.\n",
    "<br>\n",
    "The confusion matrix is a square matrix of size number_class x number_class.\n",
    "<br>\n",
    "The element `[i, j]` contains the count of the element of class `i` that were predicted as being of class `j`.\n",
    "\n",
    "Let us use the implementation of `sklearn`.\n",
    "\n",
    "\n",
    "分類結果の便利な表現の一つは__混同行列__です。\n",
    "<br>\n",
    "混同行列は、サイズ<クラス数>x<クラス数>の正方行列です。\n",
    "<br>\n",
    "行列の要素`[i, j]`には、クラス`j`であると予測されたクラス`i`の要素の数が入っています。\n",
    "\n",
    "`sklearn`の実装を使用してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "CM = confusion_matrix(y_test, y_pred)\n",
    "print(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the confusion matrix is a `3x3` matrix.\n",
    "\n",
    "The terms on the diagonal correspond to the correct predictions.\n",
    "\n",
    "この場合、混同行列は「3x3」の行列です。\n",
    "\n",
    "対角線上の要素は、正しい予測です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(\"The number of correctly predicted {} is given by CM[{},{}] = {} \\n\".format(i, i, i, CM[i, i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the accuracy can be calculated from the confusion matrix, by calculating as the ratio of the sum of the diagonal terms to the sum of all the terms.\n",
    "\n",
    "注意：混同行列の対角の要素の合計とすべての要素の合計の比率を計算すれば分類の精度が得られます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.sum(np.diag(CM)) / np.sum(CM)\n",
    "print(\"Accuracy: {:.02f}\".format(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try  it yourself ! / 自分で試そう！\n",
    "\n",
    "[Click here](session4-playground2.ipynb) to open a sample notebook and train your own classifier\n",
    "\n",
    "[ここをクリックして](session4-playground2.ipynb)、サンプルのノートブックを開き、自分で分類器を学習させてみましょう\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Learn more / 更に学ぶ (optional)\n",
    "\n",
    "#### Other measures of performance / パフォーマンスの他の測定方法\n",
    "\n",
    "Not all classes were equaly well predicted.\n",
    "<br>\n",
    "Thus, it is also usefull to give performance index per class.\n",
    "\n",
    "In addition to the accuracy, other metrics of interest are:\n",
    "- __precision__: the ratio of predictions for a class that are correct,\n",
    "- __recall__: the ratio of samples from a class that are correctly predicted,\n",
    "- __f-score__: the geometric mean of precision and recall \n",
    "\n",
    "Wikipedia: https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "\n",
    "以上はすべてのクラスが等しく予測されていたわけではありません。\n",
    "<br>\n",
    "クラスごとのパフォーマンスインデックスを指定できれば役立ちます。\n",
    "\n",
    "精度以外、関心のある他のパフォーマンス測定方法は次のとおりです。\n",
    "- __適合率__：正しいクラスの予測の比率、\n",
    "- __再現率__：正しく予測されたクラスのサンプルの比率、\n",
    "- __f値__：精度と再現率の幾何平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.diag(CM) / np.sum(CM, axis = 0)\n",
    "R = np.diag(CM) / np.sum(CM, axis = 1)\n",
    "F = 2.0 * R * P / (R + P) # 1 / ((1/P + 1/R) / 2 ) = 2 * R * P / (R + P)\n",
    "for i in range(3):\n",
    "    print(\"Class '{}' : P = {:.02f} R = {:.02f}  F = {:.02f}\".format(i, P[i], R[i], F[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation / 相互検証\n",
    "\n",
    "Since the samples are selected at random, for different trials the obtained accuracy can be different. Try this: select the cell above with the title \"Confusion matrix\" and run `Cell->Run All Above` from the menu. Do it several times, and see how the accuracy changes.\n",
    "\n",
    "In order to reduce the influence the sample selection on the accuracy result, it is often good to use __cross-validation__.\n",
    "<br>\n",
    "In cross-validation, the random selection process is repeated several times and the results are averaged.\n",
    "\n",
    "Cross-validation is done by first spliting the data into `k` equal parts (also called folds). Then the training is done with `k-1` parts and predicted on the remaining part. This is repeated for all `k` times and the results are averaged.\n",
    "\n",
    "This can be done manually, but `sklearn` also provides a function for that.\n",
    "\n",
    "サンプルはランダムに選択されるため、試行ごとに得られる精度は異なる場合があります。 これを試してください：タイトルが「混同行列」の上のセルを選択し、メニューから「セル->すべて上に実行」を実行します。 それを数回行い、精度がどのように変化するかを確認します。\n",
    "\n",
    "精度の結果に対するサンプルの選択の影響を減らすために、__相互検証__を使用することはよくあります。\n",
    "<br>\n",
    "相互検証では、ランダム選択プロセスが数回繰り返され、結果が平均化されます。\n",
    "\n",
    "相互検証は、最初にデータを`k`の等しい部分（この部分は一般「分割」と呼ばれる）に分けることによって行われます。次に、トレーニングは`k-1`部分で行われ、残りの部分で予測されます。これは`k`回繰り返し、平均をとって結果を計算します。\n",
    "\n",
    "これは手動で行うことができますが、`sklearn`はそのための機能も提供します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Reloal the dataset\n",
    "X = iris[:,1:]\n",
    "y = iris[:,0]\n",
    "\n",
    "# Create a classifier\n",
    "linear_SVC = LinearSVC()\n",
    "\n",
    "# Select the number of parts (folds)\n",
    "number_of_parts = 2\n",
    "\n",
    "# Compute the average and std of accuracy\n",
    "A = cross_val_score(linear_SVC, X, y, cv=number_of_parts, scoring=\"accuracy\")\n",
    "print(\"Accuracy: {:.02f} +/- {:.02f}\".format(A.mean(), A.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the above cell several times. See how there is little variation in the result.\n",
    "\n",
    "Wikipedia: https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "\n",
    "上記のセルを数回実行し、結果にほとんど変化がないことを確認してください。\n",
    "\n",
    "ウィキペディア：https://ja.wikipedia.org/wiki/交差検証"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
